# LLM server
LLM_PORT=18080
LLM_HEALTH_TIMEOUT_SEC=180

# Model
MODEL_FILE="qwen2.5-7b-instruct-q4_k_m.gguf"
MODEL_URL="https://example.com/model.gguf"

# Optional (docker / linux / windows)
LLM_PROFILE=cpu

# --------------------------------------------------
# macOS native llama.cpp (Metal)
# --------------------------------------------------
# Context length (smaller = faster on CPU/Metal)
LLM_CTX=2048
# Threads (P-core 위주, M1/M2 기준 6~8 권장)
LLM_THREADS=6
# Batch size (prompt 처리 속도에 영향)
LLM_BATCH=64
# Metal GPU offload layers
# 0 = CPU only, large value = as much as possible
LLM_NGL=999