#!/usr/bin/env bash
set -euo pipefail

SCRIPT_DIR="$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" && pwd)"
LLM_DIR="$SCRIPT_DIR"

ENV_FILE="$LLM_DIR/.env"
COMPOSE_FILE="$LLM_DIR/compose.yml"
MODELS_DIR="$LLM_DIR/models"

cd "$LLM_DIR"

need_cmd() {
  if ! command -v "$1" >/dev/null 2>&1; then
    echo "âŒ ERROR: $1 not found."
    exit 1
  fi
}

docker_running() {
  docker info >/dev/null 2>&1
}

trim_cr() {
  printf '%s' "$1" | sed -e 's/\r$//'
}

env_get() {
  local key="$1"
  local file="$2"
  [ -f "$file" ] || return 0

  awk -v k="$key" '
    /^[[:space:]]*#/ { next }
    /^[[:space:]]*$/ { next }
    {
      line=$0
      sub(/\r$/, "", line)
      if (index(line, k "=") == 1) {
        sub(("^" k "="), "", line)
        print line
        exit
      }
    }
  ' "$file"
}

getv() {
  local key="$1"
  local def="${2:-}"
  local val="${!key:-}"

  if [ -n "${val:-}" ]; then
    echo "$val"
    return
  fi

  val="$(env_get "$key" "$ENV_FILE" || true)"
  val="$(trim_cr "${val:-}")"

  if [ -n "${val:-}" ]; then
    echo "$val"
  else
    echo "$def"
  fi
}

compose() {
  local project
  project="$(getv LLM_PROJECT llm)"

  docker compose \
    -p "$project" \
    --project-directory "$LLM_DIR" \
    --env-file "$ENV_FILE" \
    -f "$COMPOSE_FILE" \
    --ansi never \
    "$@"
}

detect_profile() {
  local p
  p="$(getv LLM_PROFILE cpu)"
  case "$p" in
    cpu|gpu) echo "$p" ;;
    auto)
      if command -v nvidia-smi >/dev/null 2>&1; then
        if docker run --rm --gpus all nvidia/cuda:12.4.1-base-ubuntu22.04 nvidia-smi >/dev/null 2>&1; then
          echo "gpu"
          return
        fi
      fi
      echo "cpu"
      ;;
    *)
      echo "cpu"
      ;;
  esac
}

download_model_if_missing() {
  local model_file model_url models_dir model_path
  models_dir="$MODELS_DIR"
  model_file="$(getv MODEL_FILE "qwen2.5-7b-instruct-q4_k_m.gguf")"
  model_url="$(getv MODEL_URL "")"

  mkdir -p "$models_dir"
  model_path="$models_dir/$model_file"

  if [ -f "$model_path" ]; then
    return 0
  fi

  if [ -z "${model_url:-}" ]; then
    echo "âŒ ERROR: Model file not found and MODEL_URL is empty."
    echo "       Expected: $model_path"
    exit 1
  fi

  echo "â¬‡ï¸  Downloading model"
  echo "   â†’ $model_path"
  echo "   ğŸŒ $model_url"

  curl -L -C - --fail --retry 3 --retry-delay 2 \
    -o "$model_path" \
    "$model_url"

  echo "âœ… Model download complete"
}

wait_health() {
  local port timeout start_ts last_out out elapsed
  port="$(getv LLM_PORT 18080)"
  timeout="$(getv LLM_HEALTH_TIMEOUT_SEC 180)"

  echo
  echo "ğŸŒ LLM endpoint: http://localhost:${port}"
  echo "â³ Waiting for health check (timeout: ${timeout}s)"

  start_ts="$(date +%s)"
  last_out=""

  while true; do
    out="$(curl -sS --max-time 2 "http://localhost:${port}/health" || true)"
    last_out="$out"

    if [ -n "$out" ] && ! echo "$out" | grep -q "Loading model"; then
      echo "âœ… LLM is ready"
      break
    fi

    elapsed="$(( $(date +%s) - start_ts ))"
    if [ "$elapsed" -ge "$timeout" ]; then
      echo "â›” ERROR: LLM did not become ready within timeout"
      echo "Last response:"
      echo "$last_out"
      echo "ğŸ” Check logs:"
      echo "   ./llm.sh logs -f"
      exit 1
    fi

    echo "â€¦ loading (${elapsed}s / ${timeout}s)"
    sleep 2
  done
}

list_llm_containers() {
  docker ps --format '{{.Names}}' | grep -E '^(llm-cpu|llm-gpu)$' || true
}

force_kill_llm_containers_if_any() {
  local still
  still="$(list_llm_containers)"
  if [ -n "$still" ]; then
    echo "âš ï¸  Containers still running after compose down. Forcing stop/rm:"
    echo "$still" | sed 's/^/ - /'
    while IFS= read -r name; do
      [ -n "$name" ] || continue
      docker rm -f "$name" >/dev/null 2>&1 || true
    done <<< "$still"
    echo "ğŸ§¹ LLM down (forced)"
  else
    echo "âœ… LLM down"
  fi
}

usage() {
  cat <<'EOF'
Usage: ./llm.sh <command> [options]

Commands:
  up                 Start LLM server (cpu/gpu/auto via LLM_PROFILE)
  down [--clean]     Stop LLM server (always force-kills llm-cpu/llm-gpu if still running)
  restart            Restart LLM server
  status             Show compose ps
  logs [-f]          Show logs

Env (read from llm/.env, or process env):
  LLM_PROJECT=llm (optional)
  LLM_PROFILE=cpu|gpu|auto
  LLM_PORT=18080
  MODEL_FILE=...
  MODEL_URL=...
  LLM_HEALTH_TIMEOUT_SEC=180
EOF
}

need_cmd docker
need_cmd curl

if ! docker_running; then
  echo "â›” ERROR: Docker is not running. Start Docker Desktop first."
  exit 1
fi

if [ ! -f "$COMPOSE_FILE" ]; then
  echo "âŒ ERROR: compose file not found: $COMPOSE_FILE"
  exit 1
fi

if [ ! -f "$ENV_FILE" ]; then
  echo "âŒ ERROR: env file not found: $ENV_FILE"
  echo "   Create llm/.env (or copy from llm/.env.example if you have one)."
  exit 1
fi

CMD="${1:-help}"
shift || true

case "$CMD" in
  up)
    download_model_if_missing
    profile="$(detect_profile)"

    # ì „í™˜ ì‹œ ì°Œêº¼ê¸° ë°©ì§€
    compose down --remove-orphans || true
    # âœ… downì´ ëª» ì¹˜ìš´ ê²Œ ìˆìœ¼ë©´ ìë™ force ì²˜ë¦¬
    force_kill_llm_containers_if_any >/dev/null 2>&1 || true

    echo "ğŸš€ Starting LLM (profile: $profile)"
    compose --profile "$profile" up -d
    compose ps
    wait_health
    echo "ğŸ‰ Done."
    ;;

  down)
    remove_orphans=0
    for arg in "$@"; do
      case "$arg" in
        --clean|--remove-orphans) remove_orphans=1 ;;
      esac
    done

    if [ "$remove_orphans" -eq 1 ]; then
      compose down --remove-orphans || true
    else
      compose down || true
    fi

    # âœ… í•­ìƒ ê°•ì œ ì •ë¦¬ê¹Œì§€ ìˆ˜í–‰
    force_kill_llm_containers_if_any
    ;;

  restart)
    "$0" down --remove-orphans
    "$0" up
    ;;

  status)
    compose ps
    ;;

  logs)
    compose logs "$@"
    ;;

  help|-h|--help|"")
    usage
    ;;

  *)
    echo "âŒ Unknown command: $CMD"
    usage
    exit 1
    ;;
esac
